import pandas as pd  # provide sql-like data manipulation tools. very handy.
from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint
from time import time
from nltk.tokenize import TweetTokenizer  # a tweet tokenizer from nltk.
from tqdm import tqdm
import numpy as np  # high dimensional vector computing library.
from copy import deepcopy
from string import punctuation
from random import shuffle
from sklearn.preprocessing import scale
from keras.models import Sequential
from keras.layers import Dense
import gensim
from gensim.models.word2vec import Word2Vec  # the word2vec model gensim class
from keras.layers import Embedding, LSTM, Dropout, Activation, GlobalMaxPooling1D, Conv1D, MaxPooling2D, Flatten, \
    Convolution1D
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from sklearn.metrics import average_precision_score
from nltk.corpus import stopwords
pd.options.mode.chained_assignment = None
stop = stopwords.words('english')
LabeledSentence = gensim.models.doc2vec.LabeledSentence  # we'll talk about this down below

tqdm.pandas(desc="progress-bar")

tokenizer = TweetTokenizer()

np.random.seed(0)

path = '/home/shaurya/datasets/AICS_Challenge_Data/'
result_path = '/home/shaurya/PycharmProjects/fake_news/'


def ingest(path_to_data):
    data = pd.read_csv(path_to_data + 'title_normtext_label.csv')
    data['label'] = data['label'].map(int)
    data = data[data['normalizedText'].isnull() == False]
    data = data[data['title'].isnull() == False]
    data = data[data['label'] <= 1]
    data.reset_index(inplace=True)
    data.drop('index', axis=1, inplace=True)
    print 'dataset loaded with shape', data.shape
    return data


def tokenize(tweet):
    try:
        tweet = unicode(tweet.decode('utf-8').lower())
        tokens = tokenizer.tokenize(tweet)
        # tokens = filter(lambda t: not t.startswith('@'), tokens)
        # tokens = filter(lambda t: not t.startswith('#'), tokens)
        # tokens = filter(lambda t: not t.startswith('http'), tokens)
        return tokens
    except:
        return 'NC'


def postprocess(data, n):
    from nltk.corpus import stopwords
    data = data.head(n)
    data['tokens_normalizedText'] = data['normalizedText'].progress_map(
        tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.
    data = data[data.tokens_normalizedText != 'NC']
    data['tokens_title'] = data['title'].progress_map(
        tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.
    data = data[data.tokens_title != 'NC']
    data.reset_index(inplace=True)
    data.drop('index', inplace=True, axis=1)
    return data


def labelizeTweets(tweets, label_type):
    labelized = []
    for i, v in tqdm(enumerate(tweets)):
        label = '%s_%s' % (label_type, i)
        labelized.append(LabeledSentence(v, [label]))
    return labelized


def load_w2v():
    import numpy as np
    # path = '/home/shaurya/datasets/WikipediaClean5Negative300Skip10/WikipediaClean5Negative300Skip10.txt' # - 91% accuracy - 92.1553876919 AUC 10 poch
    path = '/home/shaurya/datasets/glove.6B/glove.6B.300d.txt'  # 91.22% 159 epoch - better loss and accuracy
    wiki_vec = {}
    for i in tqdm(open(path)):
        m = i.split(' ')
        word = m[0]
        vec = np.asarray(m[1:])
        vec = vec.astype(np.float)
        wiki_vec[word] = vec
    return wiki_vec


def buildWordVector(tokens, size, tfidf):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += w2v_model[word].reshape((1, size)) * tfidf[word]
            count += 1.
        except KeyError:  # handling the case where the token is not
            # in the corpus. useful for testing.
            continue
    if count != 0:
        vec /= count
    return vec


w2v_model = load_w2v()

data_train = ingest(path + 'AICS_Training_Data/')
data_test = ingest(path + 'AICS_Test_Data/')

n_train = data_train.shape[0]
n_test = data_test.shape[0]
n_dim = 300

data_train = postprocess(data_train, n_train)
data_test = postprocess(data_test, n_test)

##data_train.to_pickle(path+'data_train', compression='infer')
##data_test.to_pickle(path+'data_test', compression='infer')
##
data_train = pd.read_pickle(path+'data_train',compression='infer')
data_test = pd.read_pickle(path+'data_test',compression='infer')    

n_train = data_train.shape[0]
n_test = data_test.shape[0]
n_dim = 300

data_train['tokens_normalizedText'] = data_train['tokens_normalizedText'].apply(lambda x: [item for item in x if item not in stop])
data_test['tokens_normalizedText'] = data_test['tokens_normalizedText'].apply(lambda x: [item for item in x if item not in stop])

data_train['tokens_title'] = data_train['tokens_title'].apply(lambda x: [item for item in x if item not in stop])
data_test['tokens_title'] = data_test['tokens_title'].apply(lambda x: [item for item in x if item not in stop])


data_train.to_pickle(path+'data_train', compression='infer')
data_test.to_pickle(path+'data_test', compression='infer')



x_text_train, y_train = np.array(data_train.head(n_train).tokens_normalizedText), np.array(
    data_train.head(n_train).label)
x_text_test, y_test = np.array(data_test.head(n_test).tokens_normalizedText), np.array(data_test.head(n_test).label)

print 'Labelize Train and Test'
x_text_train = labelizeTweets(x_text_train, 'TRAIN')
x_text_test = labelizeTweets(x_text_test, 'TEST')

print 'building tf-idf matrix ... TRAIN'
vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
matrix = vectorizer.fit_transform([x.words for x in x_text_train])
tfidf_normtext_train = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
print 'vocab size TRAIN:', len(tfidf_normtext_train)

normtext_w2v_train = np.concatenate([buildWordVector(z, n_dim, tfidf_normtext_train) for z in tqdm(map(lambda x: x.words, x_text_train))])
normtext_w2v_train = scale(normtext_w2v_train)


print 'building tf-idf matrix ... TEST'
vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
matrix = vectorizer.fit_transform([x.words for x in x_text_test])
tfidf_normtext_test = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
print 'vocab size TEST:', len(tfidf_normtext_test)


normtext_w2v_test = np.concatenate([buildWordVector(z, n_dim, tfidf_normtext_test) for z in tqdm(map(lambda x: x.words, x_text_test))])
normtext_w2v_test = scale(normtext_w2v_test)

# print normtext_w2v.shape
# ========================================================
# x_text, y = np.array(data.head(n).tokens_title),np.array(data.head(n).label)

x_text_train, y_train = np.array(data_train.head(n_train).tokens_title), np.array(data_train.head(n_train).label)
x_text_test, y_test = np.array(data_test.head(n_test).tokens_title), np.array(data_test.head(n_test).label)

print 'Labelize Train and Test'
x_text_train = labelizeTweets(x_text_train, 'TRAIN')
x_text_test = labelizeTweets(x_text_test, 'TEST')

print 'building tf-idf matrix ... TRAIN'
vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=1)
matrix = vectorizer.fit_transform([x.words for x in x_text_train])
tfidf_title_train = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
print 'vocab size title Train:', len(tfidf_title_train)

title_w2v_train = np.concatenate([buildWordVector(z, n_dim, tfidf_title_train) for z in tqdm(map(lambda x: x.words, x_text_train))])
title_w2v_train = scale(title_w2v_train)

print 'building tf-idf matrix ... TEST'
vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=1)
matrix = vectorizer.fit_transform([x.words for x in x_text_test])
tfidf_title_test = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
print 'vocab size title Test:', len(tfidf_title_test)

title_w2v_test = np.concatenate([buildWordVector(z, n_dim, tfidf_title_test) for z in tqdm(map(lambda x: x.words, x_text_test))])
title_w2v_test = scale(title_w2v_test)

w2v_model = None
data_train = np.concatenate((normtext_w2v_train, title_w2v_train), axis=1)
data_test = np.concatenate((normtext_w2v_test, title_w2v_test), axis=1)

#
# from scipy import spatial
# similarity = []
# for i,j in zip(normtext_w2v, title_w2v):
#    similarity.append(1 - spatial.distance.cosine(i, j))
#
# similarity =  np.asarray(similarity).reshape((data_2.shape[0],1))
#
# data_2 = np.concatenate((data_2,similarity),axis=1)
#
# print similarity.shape
x_train, _, y_train, _ = train_test_split(data_train, y_train, test_size=0)
x_test, _, y_test, _ = train_test_split(data_test,y_test, test_size=0)
print x_train.shape, y_train.shape

# Callbacks

tensorboard = TensorBoard(log_dir=result_path + "logs/{}".format(time()))
early = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='auto')
#save_model = ModelCheckpoint(result_path + 'weights_removingcosine.hdf5',
#                             monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto',
#                             period=1)

model = Sequential()
model.add(Dense(256, activation='relu', input_dim=600))
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

hist = model.fit(x_train, y_train, epochs=25, batch_size=16, verbose=1, callbacks=[tensorboard, early],validation_data=(x_test, y_test))

scores = model.evaluate(x_test, y_test, verbose=1)
print('MLP test score:', scores[0])
print('MLP test accuracy:', scores[1])
y_score = model.predict_proba(x_test)
y_score = y_score.flatten()
print y_score.flatten(), y_test
y_score = y_score.tolist()

fpr, tpr, _ = roc_curve(y_test, y_score)

df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))

auc_score = np.trapz(tpr, fpr)
print 'AUC: ', auc_score

roc_auc = auc(fpr, tpr)
#
# fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
# roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
print roc_auc


plt.plot(fpr, tpr)
plt.show()



plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange',
         lw=lw, label='MLP (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()


average_precision = average_precision_score(y_test, y_score)

print('Average precision-recall score: {0:0.2f}'.format(
    average_precision))

precision, recall, _ = precision_recall_curve(y_test, y_score)

plt.step(recall, precision, color='b', alpha=0.2,
         where='post')
plt.fill_between(recall, precision, step='post', alpha=0.2,
                 color='b')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(
    average_precision))
#
# from sklearn.metrics import recall_score
#
# recall_score(y_test, np.array(y_score).astype(int), average='macro')
#
# recall_score(y_test, np.array(y_score).astype(int), average='micro')
